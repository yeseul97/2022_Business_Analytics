## âœ 5ë²ˆì§¸ íŠœí† ë¦¬ì–¼ "Semi-Supervised Learning"
 
### <ëª©ì°¨>
#### 1. Proxy-label method : `Pseudo Label`
#### 2. Consistency regularization : `PI model`, `VAT`, `Mean Teacher`, `ICT`
#### 3. Holistic methods : `MixMatch`
#### 4. ì‹¤í—˜

---

## 1ï¸. Proxy-label method 

  : Labeled setìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ unlabeled data pointë“¤ì— labelì„ ë‹¬ì•„ì£¼ëŠ” ê¸°ë²•


  #### ğŸ“Œ Method 1. `Pseudo Label` (13')
  ![image](https://user-images.githubusercontent.com/67623921/209647081-9874fab0-5f36-43a4-91d9-a224ca76e1ef.png)
  
  ì¶œì²˜: https://sanghyu.tistory.com/177


## 2ï¸. Consistency regularization

  : Unlabeled dataì— data augmentationì„ í†µí•´ classê°€ ë°”ë€Œì§€ ì•Šì„ ì •ë„ì˜ ë³€í™”ë¥¼ ì¤¬ì„ ë•Œ, ì› ë°ì´í„°ì™€ì˜ ì˜ˆì¸¡ê²°ê³¼ê°€ ê°™ì•„ì§€ë„ë¡ unsupervised lossë¥¼ ì£¼ì–´ í•™ìŠµí•˜ê²Œ ë¨. 



  #### ğŸ“Œ Method 2. `PI model` (16')
  ![image](https://user-images.githubusercontent.com/67623921/209645936-254931a4-4b02-4496-9ef5-089c52d4cc8a.png)
  
  ì¶œì²˜: https://amitness.com/2020/07/semi-supervised-learning/
  
  
  #### ğŸ“Œ Method 3. `VAT` (17')
  ![image](https://user-images.githubusercontent.com/67623921/209646055-166bc795-a250-416f-9357-8afd04d3824b.png)
  
  ì¶œì²˜: https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92
    
  
  #### ğŸ“Œ Method 4. `Mean Teacher` (17')
  ![image](https://user-images.githubusercontent.com/67623921/209646198-4661e185-e44c-4d23-af9f-8960e1eb6f13.png)
  
  ì¶œì²˜: https://velog.io/@injokim/%EC%99%95%EC%B4%88%EA%B8%89-Semi-supervised-learning-%EC%9E%85%EB%AC%B8


  #### ğŸ“Œ Method 5. `ICT` (19')
  ![image](https://user-images.githubusercontent.com/67623921/209646639-8695ddbc-e433-4a9f-9884-c1e03f1c245a.png)
  
  ì¶œì²˜: Verma, V., Kawaguchi, K., Lamb, A., Kannala, J., Bengio, Y., & Lopez-Paz, D. (2019). Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825.
  
  
  
## 3ï¸. Holistic methods

  : ì—¬ëŸ¬ semi-supervised learning ê¸°ë²•ë“¤ì„ í†µí•©í•˜ê³  Mixup data augmentatinoì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë” ëŒì–´ ì˜¬ë¦¼.

  #### ğŸ“Œ Method 6. `MixMatch` (19')
  ![image](https://user-images.githubusercontent.com/67623921/209646904-61c6481e-7192-4495-9139-a8ed37d9ae9b.png)
  
  ì¶œì²˜: https://rain-bow.tistory.com/entry/Semi-Supervised-Learning-SSL-%EC%86%8C%EA%B0%9C-%EB%B0%8F-%EB%8F%99%ED%96%A5


  
## 4. ì‹¤í—˜

#### `"Lossë¥¼ ë³€ê²½í•´ ê°€ë©° ì„±ëŠ¥ ë³€í™”ë¥¼ í™•ì¸í•˜ì"`
 - ê¸°ì¡´: MSE
 - L1 Loss
 - KL divergence
 - JS divergence

#### Motivation
![image](https://user-images.githubusercontent.com/67623921/209648099-730d6303-d2e7-44d1-b3db-f95a1f933a5e.png)

- ëŒ€ê·œëª¨ì˜ teacher modelì˜ ì§€ì‹ì„ ê°€ë²¼ìš´ student modelì— ì „ì´ì‹œí‚¤ëŠ” Knowledge distillation(KD) ë°©ë²•ì—ì„œëŠ” KL-div ì„ ì´ìš©í•¨. 

- KL-divergence lossëŠ” penultimate layer(softmax ì´ì „)ì˜ representationì„ ê°€ëŠ˜ê²Œ(?) í•˜ëŠ” ë°˜ë©´, MSE lossëŠ” ì´ëŸ° í˜„ìƒì„ ë³´ì´ì§€ ì•ŠìŒ.

  -softmax ì´ì „ì˜ representationì„ ë‹¤ì´ë ‰íŠ¸í•˜ê²Œ ë°°ìš°ëŠ” MSE lossì— ë¹„í•´ ì ë‹¹íˆ softmax distribution ê°„ì˜ ê±°ë¦¬ë§Œ ì¤„ì—¬ë„ ë˜ëŠ” KL-Divergence lossëŠ” ë‹¤ì±„ë¡œìš´(ì¦‰, high varianceë¥¼ ê°–ëŠ”?) representationì„ ë°°ìš¸ í•„ìš”ì„±ì„ ëª»ëŠë‚€ë‹¤ê³  ë³´ë©´ ë ë“¯.
  
- íŠ¹íˆ, teacherì™€ studentê°„ì˜ capacity gapì´ í´ ê²½ìš° KL-divergence lossë¥¼ ì´ìš©í•´ í•™ìŠµí•œ ë‹¤ìŒ, MSE-lossë¥¼ ì´ìš©í•´ ì´ì–´ì„œ í•™ìŠµí•˜ëŠ”, Sequenctial distillation í•™ìŠµì„ í•˜ëŠ” ê²Œ ë” íš¨ê³¼ì ì´ì—ˆìŒ.

- ë‹¨, labelì— ë…¸ì´ì¦ˆê°€ ë§ì„ ìˆ˜ë¡ Directí•˜ê²Œ logit matchingì„ ë°°ìš°ëŠ” MSE Lossë³´ë‹¤ ì†Œê·¹ì ìœ¼ë¡œ ë°°ìš°ëŠ” (Ï„ê°€ ë‚®ì€) KL-divergence lossë¥¼ ì“°ëŠ”ê²Œ bad trainingì˜ ì•…ì˜í–¥ì„ ì¡°ê¸ˆ ì¤„ì´ê¸´ í•¨.

