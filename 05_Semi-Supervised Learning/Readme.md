## âœ 5ë²ˆì§¸ íŠœí† ë¦¬ì–¼ "Semi-Supervised Learning"
 
### <ëª©ì°¨>
#### 1. Proxy-label method : `Pseudo Label`
#### 2. Consistency regularization : `PI model`, `VAT`, `Mean Teacher`, `ICT`
#### 3. Holistic methods : `MixMatch`
#### 4. ì‹¤í—˜

---

## 1ï¸. Proxy-label method 

  : Labeled setìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ unlabeled data pointë“¤ì— labelì„ ë‹¬ì•„ì£¼ëŠ” ê¸°ë²•


  #### ğŸ“Œ Method 1. `Pseudo Label` (13')
  ![image](https://user-images.githubusercontent.com/67623921/209647081-9874fab0-5f36-43a4-91d9-a224ca76e1ef.png)
  
  ì¶œì²˜: https://sanghyu.tistory.com/177


## 2ï¸. Consistency regularization

  : Unlabeled dataì— data augmentationì„ í†µí•´ classê°€ ë°”ë€Œì§€ ì•Šì„ ì •ë„ì˜ ë³€í™”ë¥¼ ì¤¬ì„ ë•Œ, ì› ë°ì´í„°ì™€ì˜ ì˜ˆì¸¡ê²°ê³¼ê°€ ê°™ì•„ì§€ë„ë¡ unsupervised lossë¥¼ ì£¼ì–´ í•™ìŠµí•˜ê²Œ ë¨. 



  #### ğŸ“Œ Method 2. `PI model` (16')
  ![image](https://user-images.githubusercontent.com/67623921/209645936-254931a4-4b02-4496-9ef5-089c52d4cc8a.png)
  
  ì¶œì²˜: https://amitness.com/2020/07/semi-supervised-learning/
  
  
  #### ğŸ“Œ Method 3. `VAT` (17')
  ![image](https://user-images.githubusercontent.com/67623921/209646055-166bc795-a250-416f-9357-8afd04d3824b.png)
  
  ì¶œì²˜: https://sh-tsang.medium.com/review-virtual-adversarial-training-vat-4b3d8b7b2e92
    
  
  #### ğŸ“Œ Method 4. `Mean Teacher` (17')
  ![image](https://user-images.githubusercontent.com/67623921/209646198-4661e185-e44c-4d23-af9f-8960e1eb6f13.png)
  
  ì¶œì²˜: https://velog.io/@injokim/%EC%99%95%EC%B4%88%EA%B8%89-Semi-supervised-learning-%EC%9E%85%EB%AC%B8


  #### ğŸ“Œ Method 5. `ICT` (19')
  ![image](https://user-images.githubusercontent.com/67623921/209646639-8695ddbc-e433-4a9f-9884-c1e03f1c245a.png)
  
  ì¶œì²˜: Verma, V., Kawaguchi, K., Lamb, A., Kannala, J., Bengio, Y., & Lopez-Paz, D. (2019). Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825.
  
  
  
## 3ï¸. Holistic methods

  : ì—¬ëŸ¬ semi-supervised learning ê¸°ë²•ë“¤ì„ í†µí•©í•˜ê³  Mixup data augmentationì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë” ëŒì–´ ì˜¬ë¦¼.

  #### ğŸ“Œ Method 6. `MixMatch` (19')
  ![image](https://user-images.githubusercontent.com/67623921/209646904-61c6481e-7192-4495-9139-a8ed37d9ae9b.png)
  
  ì¶œì²˜: https://rain-bow.tistory.com/entry/Semi-Supervised-Learning-SSL-%EC%86%8C%EA%B0%9C-%EB%B0%8F-%EB%8F%99%ED%96%A5


  
## 4. ì‹¤í—˜

#### `"Lossë¥¼ ë³€ê²½í•´ ê°€ë©° ì„±ëŠ¥ ë³€í™”ë¥¼ í™•ì¸í•˜ì"`
ê¸°ì¡´ Loss: `MSE` â†’ ì‹¤í—˜í•  Loss: `L1 Loss`, `KL divergence`, `JS divergence`

- Semi-Supervised Learningì˜ ëª©ì í•¨ìˆ˜ëŠ” supervised Loss $L_s$ì™€ unsupervised Loss $L_u$ì˜ í•©ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒ. 

- ì´ ì ì€ 2-stageë¡œ í•™ìŠµí•˜ëŠ” self-supervised learning, transfer learning ë“±ê³¼ì˜ ì°¨ì´ì ì„. 

- ë”°ë¼ì„œ, Semi-Supervised Learningì—ì„œëŠ” ëŒ€ìš©ëŸ‰ unlabeled dataì— ì£¼ëŠ” unsupervised taskë¥¼ ì–´ë–»ê²Œ ì •í•˜ëŠ”ì§€ê°€ ì¤‘ìš”í•¨. 

- í•˜ì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ë“¤ì´ MSEë¡œ Lossë¥¼ ê³„ì‚°í•¨. 

Kim, T., Oh, J., Kim, N., Cho, S., & Yun, S. Y. (2021). Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation. arXiv preprint arXiv:2105.08919.

- ìœ„ ë…¼ë¬¸ì„ ë³´ë©´, 



